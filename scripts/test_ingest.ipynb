{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser.file.bulk import SimpleDirectoryReader\n",
    "from parser.open_ai_func import call_openai_api, get_user_permission\n",
    "from parser.schema.base import Document\n",
    "from parser.token_func import group_split\n",
    "\n",
    "\n",
    "def metadata_from_filename(title):\n",
    "    return {'title': title}\n",
    "\n",
    "\n",
    "directory = [\"./inputs/iai_doc/\"]\n",
    "# directory = [\"./inputs/iai_sample_notebook/\"]\n",
    "min_tokens = 250\n",
    "max_tokens = 2000\n",
    "token_check = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "folder_counts = defaultdict(int)\n",
    "folder_names = []\n",
    "for dir_path in directory:\n",
    "    folder_name = os.path.basename(os.path.normpath(dir_path))\n",
    "    folder_counts[folder_name] += 1\n",
    "    if folder_counts[folder_name] > 1:\n",
    "        folder_name = f\"{folder_name}_{folder_counts[folder_name]}\"\n",
    "    folder_names.append(folder_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./inputs/iai_doc/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iai_doc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_files\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "\n",
    "doc_reader = SimpleDirectoryReader(input_dir=dir_path, input_files=None, recursive=True,\n",
    "                                 required_exts=[\".rst\", \".md\"], num_files_limit=None,\n",
    "                                 exclude_hidden=True, file_metadata=metadata_from_filename)\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "file_lst = doc_reader.input_files\n",
    "docs = []\n",
    "for file_ii in file_lst:\n",
    "    with open(file_ii, \"r\") as f:\n",
    "        md_text = f.read()\n",
    "    docs_file = markdown_splitter.create_documents([md_text])\n",
    "    docs_file = [d for d in docs_file if len(d.page_content) > 5]\n",
    "    for d in docs_file:\n",
    "        d.metadata.update({\"title\": file_ii.name})\n",
    "    docs.extend(docs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "761"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Errors\\nThe Kittn API uses the following error codes:', metadata={'title': '_errors.md'}),\n",
       " Document(page_content='Error Code | Meaning\\n---------- | -------\\n400 | Bad Request -- Your request is invalid.\\n401 | Unauthorized -- Your API key is wrong.\\n403 | Forbidden -- The kitten requested is hidden for administrators only.', metadata={'title': '_errors.md'}),\n",
       " Document(page_content=\"404 | Not Found -- The specified kitten could not be found.\\n405 | Method Not Allowed -- You tried to access a kitten with an invalid method.\\n406 | Not Acceptable -- You requested a format that isn't json.\", metadata={'title': '_errors.md'}),\n",
       " Document(page_content=\"410 | Gone -- The kitten requested has been removed from our servers.\\n418 | I'm a teapot.\\n429 | Too Many Requests -- You're requesting too many kittens! Slow down!\\n500 | Internal Server Error -- We had a problem with our server. Try again later.\", metadata={'title': '_errors.md'}),\n",
       " Document(page_content=\"503 | Service Unavailable -- We're temporarily offline for maintenance. Please try again later.\", metadata={'title': '_errors.md'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(docs)):\n",
    "    print(docs[ii].page_content)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_files\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "doc_reader = SimpleDirectoryReader(input_dir=dir_path, input_files=None, recursive=True,\n",
    "                                 required_exts=[\".rst\", \".md\"], num_files_limit=None,\n",
    "                                 exclude_hidden=True, file_metadata=metadata_from_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = doc_reader.file_extractor[\".md\"]\n",
    "parser.init_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Running AWS Batch jobs with the SDK',\n",
       "  '\\nThis tutorial assumes that you have configured the AWS Batch requirements, such as roles and permissions, as described in . \\n\\nFederated learning models are trained through sessions. You define the parameters required to train a federated model, including data and model configurations, in a session. Additional session parameters are required when using AWS Batch. \\n\\nUse the `integrateai_batch_client.ipynb` notebook to follow along and test the examples shown below by filling in your own variables as required.\\n\\nPrepare your model configuration and data schema. See  for details on this process. \\n\\nDefine your training session as usual. The session definition is passed to the batch through the task group that also contains the tasks for the batch.\\n\\n```python\\ntraining_session = client.create_fl_session(\\n    name=\"Testing notebook\",\\n    description=\"I am testing a batch job through a notebook\",\\n    min_num_clients=2,\\n    num_rounds=2,\\n    package_name=\"iai_ffnet\",\\n    model_config=,\\n    data_config=,\\n)\\n```\\n\\nImportant: The `min_num_clients` specified here **must** match the number of tasks added to the task group.\\n\\nSpecify the `model_config` and `data_config` names for the configuration and schema that you want to use.\\n\\n'),\n",
       " ('Specify batch parameters',\n",
       "  '\\nIn addition to the session definition, there are AWS Batch-specific parameters required by the SDK to run a batch job. \\nTraining and test data paths\\nSpecify the path(s) to your training and test data on S3.\\n\\n```python\\ntrain_path1 = \"s3://{path to training data}\"\\ntrain_path2 = \"s3://{path to training data}\"\\ntest_path = \"s3://{path to test data}\"\\n```\\n\\nWS Authentication\\nIf you are generating temporary AWS Credentials, specify them as in the example below. Otherwise, use the default profile credentials, or pass in a Dict of AWS credential values. \\nExample:\\naws_creds = {\\n    \\'ACCESS_KEY\\': os.environ.get(\"AWS_ACCESS_KEY_ID\"),\\n    \\'SECRET_KEY\\': os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\\n    \\'SESSION_TOKEN\\': os.environ.get(\"AWS_SESSION_TOKEN\"),\\n    \\'REGION\\': os.environ.get(\"AWS_REGION\"),\\n}\\n\\nBatch environment \\nSpecify the name of the job_queue, job_definition, and ssm_token name that you created in .\\njob_queue=\\'\\'\\njob_def=\\'\\'\\nssm_token=\\'\\'\\n\\nRunning a batch with task group and taskbuilder\\n\\nInstead of running the integrate.ai client directly, import and use the taskgroup and taskbuilder functions. For each task, create a task object. Use the taskgroup to add each task to the batch.\\n\\nOne task is equivalent to one client in integrate.ai terms. The `min_num_clients` given in the training session definition must match the number of tasks defined in the batch.\\n\\nImport the required functions.\\n\\n```python\\nfrom integrate_ai_sdk.taskgroup.taskbuilder import aws as taskbuilder_aws\\nfrom integrate_ai_sdk.taskgroup.base import SessionTaskGroup\\n```\\n\\nCreate a taskbuilder object, and provide the required parameters. \\n\\n```python\\ntb = taskbuilder_aws.batch(ssm_token_parameter_key=ssm_token, \\n    job_queue=,\\n    aws_credentials=,\\n    cpu_job_definition=)\\n```\\n\\n'),\n",
       " ('Create a task group and start the batch',\n",
       "  \"\\nThe task group defines the training_session and the tasks to run for the batch. The following code snippet creates the task group and starts the job.\\n\\n```python\\ntask_group_context = SessionTaskGroup(training_session)\\\\\\n    .add_task(tb.hfl\\n        (train_path=train_path1, \\n        test_path=test_path, \\n        session=training_session, \\n        vcpus='2', \\n        memory='16384')\\n        )\\\\\\n    .add_task(tb.hfl\\n        (train_path=train_path2, \\n        test_path=test_path, \\n        session=training_session, \\n        vcpus='2', \\n        memory='16384')\\n    ).start()\\n```\\n\\nThe vcpu and memory parameters are optional. Use them to adjust the values in the job definition if necessary. \\n\\n\\n\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_tups(file_lst[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = doc_reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(text=\"\\n\\nErrors\\nThe Kittn API uses the following error codes:\\n\\n\\nError Code | Meaning\\n---------- | -------\\n400 | Bad Request -- Your request is invalid.\\n401 | Unauthorized -- Your API key is wrong.\\n403 | Forbidden -- The kitten requested is hidden for administrators only.\\n404 | Not Found -- The specified kitten could not be found.\\n405 | Method Not Allowed -- You tried to access a kitten with an invalid method.\\n406 | Not Acceptable -- You requested a format that isn't json.\\n410 | Gone -- The kitten requested has been removed from our servers.\\n418 | I'm a teapot.\\n429 | Too Many Requests -- You're requesting too many kittens! Slow down!\\n500 | Internal Server Error -- We had a problem with our server. Try again later.\\n503 | Service Unavailable -- We're temporarily offline for maintenance. Please try again later.\\n\\n\", doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/_errors.md'}),\n",
       " Document(text='\\n\\nAWS Batch and Fargate Manual Setup\\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'}),\n",
       " Document(text='\\n\\nRequirements\\n\\nThis section outlines the setup steps required to configure your working environment. Steps that are performed in the AWS platform are not explained in detail. Refer to the AWS documentation as needed. \\n\\nThe requirements are tool-agnostic - that is, you can complete the steps through the AWS console, or through a tool such as Terraform or AWS CloudFormation. \\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'}),\n",
       " Document(text='\\n\\nAuthentication\\n\\nComplete the steps to create an IAI access token, hereafter referred to as .\\n\\nOn the AWS CLI, set the token as a parameter for your SSM agent. SSM handles getting and using the token as needed for the batch session.\\n\\n```python\\naws ssm put-parameter --name iai-token --value  --type SecureString\\n```\\n\\nExample response:\\n\\n```\\n{\\n    \"Version\": 1,\\n    \"Tier\": \"Standard\"\\n}\\n```\\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'}),\n",
       " Document(text='\\n\\nAbout \"secrets\"\\n\\nIn order for the batch job to access the integrate.ai JWT through SSM it needs to be set in the job definition secrets configuration. \\nSet the name of the secret to IAI_TOKEN to create the IAI_TOKEN environment variable that the docker client uses to authenticate with the session. \\nThe valueFrom is the SSM key that contains the integrate.ai access token. If you are running the batch job in the same region that the SSM parameter was created in, pass in the name of the SSM parameter. If the SSM parameter is in a different region, pass in the SSM parameter ARN. \\nTo have different tokens for different user groups, you must have a different batch job definition for each token.\\nThe command and parameter values are examples only. They are overwritten when starting a batch job with the SDK. \\nBatch job configuration is now complete. \\n\\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping small documents\n",
      "Separating large documents\n"
     ]
    }
   ],
   "source": [
    "# Here we split the documents, as needed, into smaller chunks.\n",
    "# We do this due to the context limits of the LLMs.\n",
    "processed_docs = group_split(documents=raw_docs, min_tokens=min_tokens, max_tokens=max_tokens, token_check=token_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(text=\"\\n\\nErrors\\nThe Kittn API uses the following error codes:\\n\\n\\nError Code | Meaning\\n---------- | -------\\n400 | Bad Request -- Your request is invalid.\\n401 | Unauthorized -- Your API key is wrong.\\n403 | Forbidden -- The kitten requested is hidden for administrators only.\\n404 | Not Found -- The specified kitten could not be found.\\n405 | Method Not Allowed -- You tried to access a kitten with an invalid method.\\n406 | Not Acceptable -- You requested a format that isn't json.\\n410 | Gone -- The kitten requested has been removed from our servers.\\n418 | I'm a teapot.\\n429 | Too Many Requests -- You're requesting too many kittens! Slow down!\\n500 | Internal Server Error -- We had a problem with our server. Try again later.\\n503 | Service Unavailable -- We're temporarily offline for maintenance. Please try again later.\\n\\n\", doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/_errors.md'}),\n",
       " Document(text='\\n\\nAWS Batch and Fargate Manual Setup\\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'}),\n",
       " Document(text='\\n\\nRequirements\\n\\nThis section outlines the setup steps required to configure your working environment. Steps that are performed in the AWS platform are not explained in detail. Refer to the AWS documentation as needed. \\n\\nThe requirements are tool-agnostic - that is, you can complete the steps through the AWS console, or through a tool such as Terraform or AWS CloudFormation. \\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'}),\n",
       " Document(text='\\n\\nAuthentication\\n\\nComplete the steps to create an IAI access token, hereafter referred to as .\\n\\nOn the AWS CLI, set the token as a parameter for your SSM agent. SSM handles getting and using the token as needed for the batch session.\\n\\n```python\\naws ssm put-parameter --name iai-token --value  --type SecureString\\n```\\n\\nExample response:\\n\\n```\\n{\\n    \"Version\": 1,\\n    \"Tier\": \"Standard\"\\n}\\n```\\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'}),\n",
       " Document(text='\\n\\nAbout \"secrets\"\\n\\nIn order for the batch job to access the integrate.ai JWT through SSM it needs to be set in the job definition secrets configuration. \\nSet the name of the secret to IAI_TOKEN to create the IAI_TOKEN environment variable that the docker client uses to authenticate with the session. \\nThe valueFrom is the SSM key that contains the integrate.ai access token. If you are running the batch job in the same region that the SSM parameter was created in, pass in the name of the SSM parameter. If the SSM parameter is in a different region, pass in the SSM parameter ARN. \\nTo have different tokens for different user groups, you must have a different batch job definition for each token.\\nThe command and parameter values are examples only. They are overwritten when starting a batch job with the SDK. \\nBatch job configuration is now complete. \\n\\n\\n', doc_id=None, embedding=None, extra_info={'title': 'inputs/iai_doc/aws-batch-manual.md'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docsgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
